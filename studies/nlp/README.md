# Recommender System
## Papers
| Title | Venue | Year | Code | Review |
|-|-|-|-|-|
| [NEAR: Non-Supervised Explainability Architecture for Accurate Review-Based Collaborative Filtering](./NEAR.pdf) | IEEE | '22 | - | [review](./NEAR/) |
| [DisAPT: Discriminative Pre-trained Language Model with Adversarial Prompt Tuning for Few-shot Text Classification]() | ? | '22 | - | [review](./DisAPT/) | 
| [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) | OpenAI | '22 | - | [review](./ChatGPT) |
| [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf) | - | '22 | - | |
| [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821) | EMNLP | '21 | [code](https://github.com/princeton-nlp/SimCSE) | [review](./SimCSE/) |
| [OPTIMUS: Organizing Sentences via Pre-trained Modeling of a Latent Space](https://arxiv.org/abs/2004.04092) | EMNLP | '20 | | |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/) | NAACL | '19 | | |
| [Attention is all you need](https://arxiv.org/abs/1706.03762) | NIPS | '17 | [code](https://nlp.seas.harvard.edu/2018/04/03/attention.html) | [review](./AttentionIsAllYouNeed/) |
