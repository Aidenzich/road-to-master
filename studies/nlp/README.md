# Recommender System
## Papers
| Title | Venue | Year | Code | Review |
|-|-|-|-|-|
| [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf) | - | '22 | - | |
| [OPTIMUS: Organizing Sentences via Pre-trained Modeling of a Latent Space](https://arxiv.org/abs/2004.04092) | EMNLP | '20 | | |
| [Attention is all you need](https://arxiv.org/abs/1706.03762) | NIPS | '17 | [code](https://nlp.seas.harvard.edu/2018/04/03/attention.html) | ✔️ |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/) | NAACL | '19 | | |
| [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821) | EMNLP | '21 | | [review](./SimCSE/) |
| DisAPT: Discriminative Pre-trained Language Model with Adversarial Prompt Tuning for Few-shot Text Classification | x | '21 | x | [review](./DisAPT/) | 
