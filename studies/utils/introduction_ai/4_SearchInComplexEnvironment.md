# Chapter 4. Search In Complex Environment
- https://moodle.ncku.edu.tw/pluginfile.php/822201/mod_resource/content/1/Chapter%204%20Search%20in%20Complex%20Environments.pdf
## Local Search Algorithms and Optimization Algorithms
### Hill Climbing Search
![](https://i.imgur.com/IGy98j7.png)

- Called **Greedy Local Search**
    - it grabs a good neighbor state without thinking ahead about where to go next.
    - follows the problem-solving heuristic of making the **locally optimal** choice at each stage.
- Stuck for
    - Local maximum
    - Ridges
    - Plateaux
        - a flat local maximum
        - shoulder
- The success of hill climbing depends on the shap of the state space landscape
#### Stochastic Hill Climbing
- chooses at random from among the uphill moves
- the probability of selection can vary with the steepness of the uphill move
#### First-Choice Hill Climbing
- implement from [Stochastic Hill  Climbing](####Stochastic-Hill-Climbing)
- Climbing by *generating successors randomly*  until one successor better than the current state.
#### Random-restart hill climbing
- conducts a series of hill-climbing searches from ***randomly generated initial states*** until a goal is found.


### Simulated Annealing 
![](https://i.imgur.com/Z5QFWUL.gif)

- **Annealing**:
    - allowing the material to reach a low-energy **crystalline state**
- Algorithm
    $$ \Delta E  = next.Value - current.Value $$
    if $\Delta E > 0$ then current is next
    else current == next only with probability $e^{\Delta E/T}$
- It **picks a random move** instead of picking the best move.
    - If the move improves the situation, it's always accepted.
- The algorithm accepts the move with some probability less than 1.
    - $P_{bad}\uparrow \quad$ when $T$ is high.
    - $P_{bad}\downarrow \quad$ when $T$ is down.
    - $T$ means Temperature, or the situation states.
    - The probability decreases exponentially with the “badness” of the move.
### Local Beam Search
- keeps track of $k$ states rather than just 1.
- Process:
    - It begins with $k$ randomly generated states.
    - At each step, the successors of all $k$ states are generated.
        ```
        if (any one is a goal){
            // the algorthm halts
        } else {
            // selects k best successors 
            // from the complete list & repeats.
        }
        ```

#### Stochastic Beam Search 
- chooses k successors at random
- with the probability of choosing a given successor being an increasing function of its value. 
$$
    P(choosingGivenSuccessor) = f_{increasing}(Value)
$$
#### Genetic Algorithms
![](https://i.imgur.com/WOtZ21r.png)

- A variant of [*Stochastic Beam Search*](####Stochastic-Beam-Search)
    - successor states -> generated by two parent states
    - Process: 
        - **Select** states as parent.
        - Generate **Crossover** Successors.
        - Generate **Mutation** Successors.
        - **Fitness function** return higher value for better states.
        - use better states as parent and cross over again and again...
        - find the best states
    
---
### Local Search in Continuous Spaces
- Suppose 
$$
    f(x_1, y_1, x_2, y_2, x_3, y_3) = \Sigma_{i=1, c\in C}^3\Sigma (x_i - xC)^2 + (y_i - y_c)^2
$$
- Use the gradient of the landscape to find a maximun
$$
\nabla f  = (\partial f/ \partial x_1, \partial f/ \partial y_1, \partial f/ \partial x_2, \partial f/ \partial y_2, \partial f/ \partial x_3, \partial f/ \partial y_3)
$$
- Perform steepestascent hill climbing by updateing the current state according to 
    $$ x = x + \alpha \nabla f(x) $$
    - $\alpha$ is a small constant often called the ***step size***, in machine learning it also called ***learning rate***
        - $\alpha$ is too small = too many steps are needed
        - $\alpha$ is too large = the search could overshoot the maximum
- To find a maximum or minimum of $f$ (Optimization)
    - we need to find x that the gradient = 0
    - Formula:
        $$ x \leftarrow x - H_f^{-1}(x)\nabla f(x) $$
- **Constrained Optimization Problem**
    - constrained if solutions must satisfy some hard constraints
    - Linear Programing problems
        - constraint: 
            - linear inequlities forming a convex set
            - the objective function is also linear
- p.s. if you want to solve linear programing problems, *The course of optimization* is all you need
#### Newton's Method (Newton’s method of tangents)
![](https://i.imgur.com/MLAmPru.gif)
- If we draw a $tangent$ to at the given point , then the tangent line **intersects the x-axis** at the point , which we expect to be closer to the root of x.
- Newton's formula
    $$
        x \leftarrow x - g(x)/g'(x)
    $$
### Searching with Nondeterministic Actions
- Nondeterministic
    - Results function returns a **set of possible** instead of single state.
- Need to generalize the notion of a **solution to the problem**.
    - **Not** single sequence of actions.
    - It's a **contingency plan** of actions.
        - e.g. [$Suck$, if $State = 5$ then [$Right ,Suck$] else []]
        - Can contain nested **if-then-else statements** 
(They are Trees not sequences)
- Physical World are contingency problems because exact prediction is impossible.

#### AND-OR Search Trees
![](https://i.imgur.com/kiq0NHy.png)

- ***OR Nodes*** 
    - Branching is agent's own choices in each state.
- ***AND Nodes*** 
    - Branching is the environment's choice of outcome for each action.
- ***Solution for AND-OR Search Problem***
    - 1. Has a goal node at every leaf.
    - 2. Specifies one action at each OR nodes.
    - 3. Includes every outcome branch at each   AND nodes.

### Searching with Partial Observations
- Partial Observations: Agent's percepts is not suffice to understand the exact state.
- **Belief State**
    - the agent's current belief to the possible states it might be.
    - it given the sequence of actions and percepts to that point.

#### Searching with no observation (Sensorless Problem)
- Search in the space of *belief states* rather than physical states.
- Underlying Physical Problem $P$ is [defined](https://hackmd.io/n41mERj2TdywoZL0E4JlLg##Well-Defined-Problems-and-Solution) by:
    - $Action_p$
    - $Result_p$
    - $GoalTest_p$
    - $StepTest_p$
- ***Belief State***($B$)
    - Contains every possible set of ***Physical States***($P$).
        $$ 
        P \leq B
        $$
    - if $P$ has N states, $B$ has $2^N$ states.
- ***Initial State***
    - The set of all states in $P$.
    - In some cases the agent will have more information than this.
- ***Actions***
    - Suppose the agent is in belief state
        $$
            B = \{ S_1, S_2\} 
        $$
    - And 
        $$Actions(S_1) \neq Actions(S_2)$$ 
    - if illegal actions have no effect to environment, then
        $$
            Actions(B) = \bigcup\limits_{s\in B} Actions(S)
        $$
        - $\cup$  means union
#### An agent for partially observable environments
- Similar to the Simple Problem-Solving agent but with ...
- **2 main differences**
    - 1. The solution to a problem is a *conditional plan*
    - 2. The *belief state* needs to maintain due to it performs actions and receives percepts.



### Online Searching Agents with Unknown Environments
#### Offline Search
- We have concentrated on agents that use offline search algorithms which:
    - compute a complete solution before setting foot in the real world.
    - and then execute the solution.
#### Online Search
- An online search agent interleaves computation and action
    - first it takes an action.
    - then observes the environment and computes the next action.