# Natural language processing (NLP) 
## Papers
| Title | Venue | Year | Code | Review |
|-|-|-|-|-|
| [Instruction Tuning with GPT-4](https://arxiv.org/abs/2304.03277) | - | '23 | [✓](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | [-](./InstructioinTuningWithGPT4/) |
| [ChatGPT for Robotics: Design Principles and Model Abilities](https://www.microsoft.com/en-us/research/uploads/prod/2023/02/ChatGPT___Robotics.pdf) | - | '23 | [✓](https://github.com/microsoft/PromptCraft-Robotics) | [✓](./ChatGPT4Robotics/) |
| [NEAR: Non-Supervised Explainability Architecture for Accurate Review-Based Collaborative Filtering](./NEAR.pdf) | IEEE | '22 | [✓](https://github.com/IKMLab/PHD_Dissertation_Reinald_Pugoy_P78077040) | [✓](./NEAR/) |
| [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) | ICLR | '22 | [✓](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py) | [✓](./Lora/) |
| [DisAPT: Discriminative Pre-trained Language Model with Adversarial Prompt Tuning for Few-shot Text Classification]() | - | '22 | - | [-](./DisAPT/) | 
| [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) | OpenAI | '22 | - | [✓](./ChatGPT) |
| [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf) | - | '22 | - | - |
| [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821) | EMNLP | '21 | [✓](https://github.com/princeton-nlp/SimCSE) | [✓](./SimCSE/) |
| [OPTIMUS: Organizing Sentences via Pre-trained Modeling of a Latent Space](https://arxiv.org/abs/2004.04092) | EMNLP | '20 | - | - |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/) | NAACL | '19 | - | - |
| [Attention is all you need](https://arxiv.org/abs/1706.03762) | NIPS | '17 | [✓](https://nlp.seas.harvard.edu/2018/04/03/attention.html) | [✓](./AttentionIsAllYouNeed/) |
