| Property  | Data |
|-|-|
| Created | 2022-12-18 |
| Updated | 2022-12-19 |
| Author | @Aiden |
| Tags | #study |

# NEAR: Non-Supervised Explainability Architecture for Accurate Review-Based Collaborative Filtering

| Title | Venue | Year | Code |
|-|-|-|-|
| [NEAR: Non-Supervised Explainability Architecture for Accurate Review-Based Collaborative Filtering](./NEAR.pdf) | IEEE | '22 | - |


## Abstract
- **Cause:** The lack of ground-truth explanation texts for training, It is unrealistic to expect every user-item pair in a dataset to have a corresponding target explanation. 
- **Process:** We formally define two explanation types, both of which NEAR can produce. 
    | Types | Definetion |
    |-|-|
    | **Invariant Explanation** | fixed for all users, is based on the unsupervised extractive summary of an item’s reviews via embedding clustering. |
    | **Variant Explanation** | personalized for a specific user, is a sentence-level text generated by our customized Transformer conditioned on every user-item-rating tuple and **artificial ground-truth (self-supervised label)** from one of the invariant explanation’s sentences.  |
- **Effect:** we pioneer the first non-supervised explainability architecture for review-based collaborative filtering (called NEAR) as our novel contribution to the theory of explanation construction in recommender systems. 
    - While maintaining excellent recommendation performance, they approach **reformulates explainability as a non-supervised (i.e., unsupervised and self-supervised) explanation generation task**. 
    - The empirical evaluation illustrates that NEAR’s rating prediction accuracy is better than the other state-of-the-art baselines. Moreover, experiments and assessments show that NEAR-generated variant explanations are more personalized and distinct than those from other Transformer-based models, and our invariant explanations are preferred over those from other contemporary models in real life.

### Invariant Explanation
- [extractive-summarizer](https://github.com/dmmiller612/bert-extractive-summarizer)
- K-Means