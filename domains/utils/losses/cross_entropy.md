# Cross Entropy
$$
\mathcal{L}_{\text{CE}} = - \sum_{i \in K} y_i \log(p(y))
$$
| Property | Description |
|-|-|
| $y_i$ | Indicator variable |
| $p(y_i)$ | The probability of class $i$  |
| $\sum_{i \in K}$ | Sum over classes |
- [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
- input is `logit`, which is an unnormalized scores



## Binary Cross Entropy
Binary cross entropy is a type of cross entropy loss used in binary classification problems.
- It measures the `dissimilarity` between the `predicted probability distribution` and the `actual distribution` (which is either 0 or 1) for each example in the dataset.
- The `binary cross entropy loss` is computed by taking the `negative logarithm of the predicted probability for the positive class`, if the actual class is positive, or for the negative class, if the actual class is negative. 
The resulting loss values are then `summed` over all examples and averaged to produce the final binary cross entropy loss. 
- Minimizing this loss will result in the model having higher accuracy in its predictions for binary classification problems.

$$
\frac{- 1}{N} \sum_1^N {y_i \times \log(p({y}_i)) + (1-y_i) \times \log(1- p({y}_i))}
$$

- The value of $y_i$
    | label | $y_i$ |
    |-|-|
    | Positive | 1 |
    | Negative | 0 |


#### Maximum Likelihood Estimation
- Use Model to infer Data's distribution
    - We know the x, and we calculate the Probilities of Data distribution
    - Use $ln$ 
- Probability is not Likelihood
    ![](./assets/prob_likelihood.png)
    - Distribution -> Probability
    - Likelihood is probability's reverse






## Cross Entropy + Softmax æ¢¯åº¦æ¨å°å…¨ç´€éŒ„ (Grad-Backprop)

æ¥ä¸‹æˆ‘å€‘å±•ç¤º $\log$ èˆ‡ $\exp$ å¦‚ä½•åœ¨æ•¸å­¸ä¸Šå®Œç¾æŠµæ¶ˆï¼Œæœ€çµ‚ç”¢ç”Ÿæ¥µç°¡çš„ $Q - P$ æ¢¯åº¦å…¬å¼ã€‚

---

## 1. å®šç¾©èˆ‡ç›®æ¨™
è¨­å®šæ¨¡å‹è¼¸å‡ºèˆ‡çœŸå¯¦æ¨™ç±¤å¦‚ä¸‹ï¼š
* **Logits ($z$):** æ¨¡å‹æœ€å¾Œä¸€å±¤çš„åŸå§‹è¼¸å‡ºã€‚
* **Softmax ($Q$):** é æ¸¬æ©Ÿç‡ï¼Œå®šç¾©ç‚º 
    $$
    \textcolor{orange}{Q_i = \frac{e^{z_i}}{\sum_k e^{z_k}}}
    $$
* **Ground Truth ($P$):** çœŸå¯¦æ¨™ç±¤ï¼ˆå¯èƒ½æ˜¯ One-hot æˆ–å¹³æ»‘å¾Œçš„æ©Ÿç‡ï¼‰ã€‚
* **Loss ($L$):** äº¤å‰ç†µæå¤± 
    $$
    \textcolor{cyan}{L = -\sum_k P_k \ln(Q_k)}
    $$

**ç›®æ¨™ï¼š** è¨ˆç®— $\frac{\partial L}{\partial z_i}$ã€‚

---

## 2. æ ¸å¿ƒåŒ–ç°¡
å°‡ Q å¸¶å…¥ $\ln$ ä¸­

$$L = -\sum_k P_k \ln \left( \textcolor{orange}{\frac{e^{z_k}}{\sum_j e^{z_j}}} \right)$$

åˆ©ç”¨å°æ•¸æ€§è³ª $\ln(\frac{a}{b}) = \ln a - \ln b$ï¼š
$$L = -\sum_k P_k [ \underbrace{\ln(e^{z_k})}_{z_k} - \underbrace{\ln(\sum_j e^{z_j})}_{\text{LSE}} ]$$

å±•é–‹æ‹¬è™Ÿå¾Œï¼š
$$L = -\sum_{\textcolor{orange}{k}} P_k z_k + \sum_{\textcolor{orange}{k}} P_k \ln(\sum_j e^{z_j})$$

ç”±æ–¼ $\sum_k P_k = 1$ï¼ˆæ©Ÿç‡ç¸½å’Œç‚º 1ï¼‰ï¼Œå¾ŒåŠéƒ¨åˆ†çš„ $\ln(\sum e^{z_j})$ å¯ä»¥ç›´æ¥æå–å‡ºä¾†(å› ç‚ºèˆ‡ $\textcolor{orange}{k}$ ç„¡é—œ)ï¼š


$$L = \mathbf{-\sum_{\textcolor{orange}{k}} P_k z_k} + \mathbf{\ln(\sum_j e^{z_j})}$$

---

## 3. åˆ†é …åå¾®åˆ†éç¨‹
æˆ‘å€‘ç¾åœ¨å°å–®ä¸€åˆ†é‡ $z_i$ é€²è¡Œæ±‚å°ï¼š

### A. å‰åŠéƒ¨ï¼ˆç·šæ€§é …ï¼‰çš„å°æ•¸
åœ¨ $-\sum_k P_k z_k$ ä¸­ï¼Œåªæœ‰ $k=i$ çš„é‚£ä¸€é …åŒ…å« $z_i$ï¼š
$$\frac{\partial}{\partial z_i} [ -(P_1 z_1 + \dots + P_i z_i + \dots) ] = \mathbf{-P_i}$$

### B. å¾ŒåŠéƒ¨ï¼ˆLSEé …ï¼‰çš„å°æ•¸
åˆ©ç”¨é€£é–å¾‹ (Chain Rule)

> é€£é–å¾‹æ˜¯æ±‚è¤‡åˆå‡½æ•¸å°æ•¸çš„æ ¸å¿ƒå·¥å…·ã€‚ç•¶ä¸€å€‹å‡½æ•¸æ˜¯ç”±å¤šå€‹å‡½æ•¸åµŒå¥—è€Œæˆï¼Œä¾‹å¦‚ $y = f(g(x))$ï¼Œå…¶å°æ•¸ç‚ºå¤–å±¤å°æ•¸èˆ‡å…§å±¤å°æ•¸çš„ä¹˜ç©ï¼š
> $$\frac{dy}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$$
> ä»¥ $\ln(u)$ ç‚ºä¾‹ï¼Œå°æ‡‰é—œä¿‚å¦‚ä¸‹ï¼š
> * **å¤–å±¤å‡½æ•¸ $f(g)$**ï¼š$\ln(g)$ $\rightarrow$ å…¶å°æ•¸ç‚º $\frac{1}{g}$ã€‚
> * **å…§å±¤å‡½æ•¸ $g(x)$**ï¼š$u(x)$ $\rightarrow$ å…¶å°æ•¸ç‚º $u'$ã€‚
> æœ€çµ‚æˆ‘å€‘å¯ä»¥å¾—åˆ°å° $\ln(u)$ çš„å°æ•¸ç‚º:
> $$\frac{d}{dx} \ln(u) = \frac{1}{u} \cdot u'$$

å°‡å…¬å¼å¸¶å…¥ LSE:
$$
\frac{\partial}{\partial z_i} \ln(\sum_j e^{z_j}) = \frac{1}{\textcolor{cyan}{\sum_j e^{z_j}}} \cdot \textcolor{cyan}{ \frac{\partial}{\partial z_i}(\sum_j e^{z_j})}$$
ç”±æ–¼ $\sum e^{z_j}$ å° $z_i$ æ±‚å°åªå‰©ä¸‹ $e^{z_i}$ï¼Œæ•…ï¼š
$$\frac{\partial}{\partial z_i} \ln(\sum_j e^{z_j}) = \frac{e^{z_i}}{\sum_j e^{z_j}} = \mathbf{Q_i}$$

---

## 4. æœ€çµ‚çµæœï¼šå„ªé›…çš„æ®˜å·®
å°‡ A èˆ‡ B åˆä½µï¼Œå¾—åˆ°æœ€çµ‚æ¢¯åº¦ï¼š

$$ \frac{\partial L}{\partial z_i} = \mathbf{Q_i - P_i} $$

### ğŸ’¡ ç‰©ç†ç›´è¦ºè¨˜æ†¶é»
* **$Q_i$ (ç¾åœ¨åœ¨å“ª):** æ¨¡å‹ç›®å‰çš„é æ¸¬æ©Ÿç‡ã€‚
* **$P_i$ (è¦å»å“ªè£¡):** çœŸå¯¦çš„ç›®æ¨™ä½ç½®ã€‚
* **æ¢¯åº¦:** å…©è€…ä¹‹é–“çš„**å·®è· (Residual)**ã€‚
    * å¦‚æœé æ¸¬ 0.9ï¼Œç›®æ¨™ 1.0 $\to$ æ¢¯åº¦ç‚º -0.1ï¼ˆéœ€è¦å¾€æ­£æ–¹å‘å¾®èª¿ï¼‰ã€‚
    * å¦‚æœé æ¸¬ 0.1ï¼Œç›®æ¨™ 0.0 $\to$ æ¢¯åº¦ç‚º 0.1ï¼ˆéœ€è¦å¾€è² æ–¹å‘å¾®èª¿ï¼‰ã€‚

---
**é¢è©¦å‚™è¨»ï¼š** L5 é¢è©¦ä¸­è‹¥è¢«å•åŠæ­¤è™•ï¼Œå¯é¡å¤–æåˆ° $Q - P$ çš„ç°¡æ½”æ€§ä¿è­‰äº†åå‘å‚³æ’­æ™‚ä¸æœƒå‡ºç¾æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ï¼ˆåœ¨åˆæ­¥æ›´æ–°æ™‚ï¼‰ï¼Œé€™ä¹Ÿæ˜¯ Softmax + Cross Entropy æˆç‚ºé»ƒé‡‘æ­æª”çš„åŸå› ã€‚


## References
- [Explanation of Infromation Entropy](https://www.ycc.idv.tw/deep-dl_2.html)
- [Explanation of Maximum Likelihood Estimation](https://www.ycc.idv.tw/deep-dl_3.html)