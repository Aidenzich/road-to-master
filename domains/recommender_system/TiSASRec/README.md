| Property  | Data |
|-|-|
| Created | 2023-02-21 |
| Updated | 2023-02-21 |
| Author | [@Aiden](https://github.com/Aidenzich) |
| Tags | #study |

# TiSASRec
## Point-Wise Feed-Forward Network
Though our time interval aware attention layer is able to incorporate all previous items, absolute position, and relative time information with adaptive weights, it does so via a linear combination. 
After each time-aware attention layer, we apply two linear transformations with a ReLU activation in between, which could endow non-linearity to the mode

- What's the diffrence between Point-Wise and Position-wise

## Feed Forward Layer
- Change traindition feed forward from 2 linear model to 2 convoluation layer